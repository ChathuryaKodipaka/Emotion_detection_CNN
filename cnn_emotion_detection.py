# -*- coding: utf-8 -*-
"""CNN_emotion_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eRGABpw1UHuQOnaAovtnO4zh-GNa1hjE

Emotion Based Music Recommendation System
"""

!mkdir -p /root/.kaggle
!cp kaggle.json /root/.kaggle/

!kaggle datasets download -d msambare/fer2013

!unzip /content/fer2013.zip -d /content/

from google.colab import drive
drive.mount('/content/drive')

import os

# List all directories and files in the current directory
root_dir = '/content/'
folders = os.listdir(root_dir)

print("Folders and files in the root directory:")
print(folders)

import os

# Define the correct paths to your train and test directories
train_dir = '/content/train/'
test_dir = '/content/test/'

# List folders in the train and test directories
train_folders = os.listdir(train_dir)
test_folders = os.listdir(test_dir)

print("Folders in the train directory:")
print(train_folders)

print("\nFolders in the test directory:")
print(test_folders)

"""Libraries"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('default')

import os
import tensorflow as tf
import keras
import cv2

from sklearn.model_selection import train_test_split

from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.utils import plot_model
from tensorflow.keras import layers, models, optimizers

from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import *
from tensorflow.keras.applications import ResNet50V2

"""Visualization of Classes"""

def Classes_Count(path, name):
    Classes_Dict = {}

    for Class in os.listdir(path):
        Full_Path = path + Class
        Classes_Dict[Class] = len(os.listdir(Full_Path))

    df = pd.DataFrame(Classes_Dict, index=[name])
    return df

Train_Count = Classes_Count(train_dir, 'Train').transpose().sort_values(by="Train", ascending=False)
Test_Count = Classes_Count(test_dir, 'Test').transpose().sort_values(by="Test", ascending=False)

# Concatenate the train and test counts into a single DataFrame for comparison
pd.concat([Train_Count, Test_Count], axis=1)

Train_Count.plot(kind='barh')

Test_Count.plot(kind='barh')

plt.style.use('default')
plt.figure(figsize=(25, 8))
image_count = 1
BASE_URL = '/content/train/'

for directory in os.listdir(BASE_URL):
    if directory[0] != '.':
        for i, file in enumerate(os.listdir(BASE_URL + directory)):
            if i == 1:
                break
            else:
                fig = plt.subplot(1, 7, image_count)
                image_count += 1
                image = cv2.imread(BASE_URL + directory + '/' + file)
                plt.imshow(image)
                plt.title(directory, fontsize=20)

"""Data Preprocessing"""

img_shape = 48
batch_size = 64
train_data_path = '/content/train/'
test_data_path = '/content/test/'

train_preprocessor = ImageDataGenerator(
    rescale = 1 / 255.,
    # Data Augmentation
    rotation_range = 10,
    zoom_range = 0.2,
    width_shift_range = 0.1,
    height_shift_range = 0.1,
    horizontal_flip = True,
    fill_mode = 'nearest'
)

test_preprocessor = ImageDataGenerator(
    rescale = 1 / 255.
)

train_data = train_preprocessor.flow_from_directory(
    train_data_path,
    class_mode="categorical",
    target_size=(img_shape, img_shape),
    color_mode='rgb',
    shuffle=True,
    batch_size=batch_size,
    subset='training'
)
test_data = test_preprocessor.flow_from_directory(
    test_data_path,
    class_mode="categorical",
    target_size=(img_shape, img_shape),
    color_mode="rgb",
    shuffle=False,
    batch_size=batch_size
)

"""Building CNN Model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

# Model definition
def create_cnn_model():
    model = Sequential()

    # Adjust the input shape to (48, 48, 3)
    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 3)))
    model.add(BatchNormalization())
    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
    model.add(Dropout(0.25))

    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(BatchNormalization())
    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
    model.add(Dropout(0.25))

    model.add(Conv2D(128, (3, 3), activation='relu'))
    model.add(BatchNormalization())
    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
    model.add(Dropout(0.25))

    # Flatten the output of the convolutional layers
    model.add(Flatten())

    # Fully connected layers
    model.add(Dense(1024, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))

    model.add(Dense(512, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))

    # Output layer for 7 classes
    model.add(Dense(7, activation='softmax'))

    return model

cnn_model = create_cnn_model()
cnn_model.summary()

# Compile the model
cnn_model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Define callbacks
callbacks = [
    EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True),
    ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', verbose=1),
    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=1)
]

# Train the model with corrected input dimensions
CNN = cnn_model.fit(
    train_data,
    validation_data=test_data,
    epochs=200,
    batch_size=batch_size,
    callbacks=callbacks
)

import matplotlib.pyplot as plt

# Assuming you have the 'CNN' training history
history = CNN.history

# Get the values for accuracy, loss, and validation metrics
epochs = range(1, len(history['accuracy']) + 1)
accuracy = history['accuracy']
val_accuracy = history['val_accuracy']
loss = history['loss']
val_loss = history['val_loss']

# Plot the training and validation accuracy and loss
plt.figure(figsize=(15, 5))

# Plot Loss
plt.subplot(1, 2, 1)
plt.plot(epochs, loss, label="Training Loss")
plt.plot(epochs, val_loss, label="Validation Loss")
plt.title("Training and Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()

# Plot Accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs, accuracy, label="Training Accuracy")
plt.plot(epochs, val_accuracy, label="Validation Accuracy")
plt.title("Training and Validation Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()

plt.tight_layout()
plt.show()

# Identify the epoch where the best model (based on validation loss) was saved
best_epoch = CNN.history['val_loss'].index(min(CNN.history['val_loss'])) + 1

# Retrieve the training and validation accuracy for that best epoch
best_train_accuracy = CNN.history['accuracy'][best_epoch - 1]
best_val_accuracy = CNN.history['val_accuracy'][best_epoch - 1]

print(f"The best epoch based on validation loss is: {best_epoch}")
print(f"Training Accuracy at best epoch: {best_train_accuracy:.4f}")
print(f"Validation Accuracy at best epoch: {best_val_accuracy:.4f}")

from tensorflow.keras.models import load_model

# Load the best model saved by ModelCheckpoint
best_model = load_model('best_model.keras')

# Evaluate the best model on test data
test_loss, test_accuracy = best_model.evaluate(test_data)
print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Test Loss: {test_loss:.4f}")

from tensorflow.keras.models import load_model

# Step 1: Identify the best epoch based on validation loss
best_epoch = CNN.history['val_loss'].index(min(CNN.history['val_loss'])) + 1

# Step 2: Retrieve the training and validation accuracy for that best epoch
best_train_accuracy = CNN.history['accuracy'][best_epoch - 1]
best_val_accuracy = CNN.history['val_accuracy'][best_epoch - 1]

# Step 3: Load the best model saved by ModelCheckpoint
best_model = load_model('best_model.keras')

# Step 4: Evaluate the best model on test data
test_loss, test_accuracy = best_model.evaluate(test_data)

# Step 5: Print and compare all accuracies
print(f"Best Epoch: {best_epoch}")
print(f"Training Accuracy at best epoch: {best_train_accuracy:.4f}")
print(f"Validation Accuracy at best epoch: {best_val_accuracy:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

import seaborn as sns
import numpy as np
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

# Get the true labels from test data
true_labels = test_data.classes  # test_data.classes gives the true labels for the test set

# Get the predicted labels from the model
# Since predict() returns probabilities, we use np.argmax to get the class with highest probability
pred_probabilities = cnn_model.predict(test_data)
pred_labels = np.argmax(pred_probabilities, axis=1)

# Create the confusion matrix
cm = confusion_matrix(true_labels, pred_labels)

# Plot the confusion matrix
fig, ax = plt.subplots(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', ax=ax)

# Set axis labels and title
ax.set_xlabel('Predicted labels', fontsize=15, fontweight='bold')
ax.set_ylabel('True labels', fontsize=15, fontweight='bold')
ax.set_title('CNN Confusion Matrix', fontsize=20, fontweight='bold')

# Show the plot
plt.show()

# Save the trained model
cnn_model.save('cnn_model.keras')

from google.colab import files
files.download('cnn_model.keras')